{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f284cac-aaee-453d-946f-a2acb4e1cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da008c91-12a5-477c-b2fc-3b0f453e1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力机制\"\"\"\n",
    "    def __init__(self, feature_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert feature_dim % num_heads == 0, \"feature_dim必须能被num_heads整除\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        \n",
    "        # 定义QKV变换层\n",
    "        self.Wq = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wk = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wv = nn.Linear(feature_dim, feature_dim)\n",
    "        self.fc_out = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        # 缩放因子\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x形状: (batch_size, seq_len, feature_dim)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # 生成QKV\n",
    "        Q = self.Wq(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.Wk(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.Wv(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力得分\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 注意力加权\n",
    "        context = torch.matmul(attn_weights, V).transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # 输出变换\n",
    "        return self.fc_out(context)\n",
    "\n",
    "class EnhancedSequenceAttention(nn.Module):\n",
    "    \"\"\"增强版序列注意力（多头 + 残差）\"\"\"\n",
    "    def __init__(self, feature_dim, num_heads=8, output_dim=64):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = MultiHeadAttention(feature_dim, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(feature_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.reduce_dim = nn.Linear(feature_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 残差连接\n",
    "        attn_output = self.multihead_attn(x)\n",
    "        out = self.layer_norm(x + self.dropout(attn_output))\n",
    "\n",
    "        out = torch.mean(out, dim=1)  # 形状变为 (batch, 128)\n",
    "        out = self.reduce_dim(out) \n",
    "        out = self.relu(out)\n",
    "        # 形状变为 (batch, 64)\n",
    "        # 全局平均池化\n",
    "        return out  # 或使用加权求和\n",
    "class ImprovedBidirectionalGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, external_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ------------------- 时间序列部分 -------------------\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dims[0], batch_first=True, bidirectional=True)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dims[0] * 2)\n",
    "        \n",
    "        self.gru2 = nn.GRU(hidden_dims[0] * 2, hidden_dims[1], batch_first=True, bidirectional=True)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dims[1] * 2)\n",
    "        \n",
    "        self.gru3 = nn.GRU(hidden_dims[1] * 2, hidden_dims[2], batch_first=True, bidirectional=True)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dims[2] * 2)\n",
    "        \n",
    "        self.attention = EnhancedSequenceAttention(hidden_dims[2] * 2)\n",
    "        \n",
    "        # ------------------- 外部变量直接拼接 -------------------\n",
    "        # 删除原有的外部变量处理层（self.ext_fc）\n",
    "        \n",
    "        # ------------------- 联合全连接层 -------------------\n",
    "        # 调整输入维度：时间序列特征维度 + 外部变量原始维度\n",
    "        self.fc1 = nn.Linear(64 + external_dim, 32)  # 原16 → 改为 external_dim=2\n",
    "        self.ln_fc1 = nn.LayerNorm(32)\n",
    "        self.output_fc = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, ts_input, ext_input):\n",
    "        # 时间序列处理（保持不变）\n",
    "        out, _ = self.gru1(ts_input)\n",
    "        out = self.ln1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out, _ = self.gru2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out, _ = self.gru3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        \n",
    "        # 注意力层\n",
    "        ts_feature = self.attention(out)  # (batch_size, hidden_dims[3]*2)\n",
    "        \n",
    "        # ------------------- 直接拼接外部变量 -------------------\n",
    "        # 假设 ext_input 已经过标准化处理（如StandardScaler）\n",
    "        combined = torch.cat([ts_feature, ext_input], dim=1)  # (batch_size, hidden_dims[3]*2 + 2)\n",
    "        \n",
    "        # 全连接层（保持不变）\n",
    "        out = self.fc1(combined)\n",
    "        out = self.ln_fc1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return self.output_fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e3d0a-cbdf-455b-8d82-db263be873f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a6dc3-4617-4dfe-8f79-db005e7fd3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738be63-1bde-481d-a371-ae0089ecfe92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5fd353-88f1-4844-bcc6-4233903e63c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d2c36-8765-49fb-901c-3a58fb4edd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f3da7-0860-42fd-aadf-21bec46b661b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
